{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYiZq0X2oB5t"
   },
   "source": [
    "# **CSCE 5218 / CSCE 4930 Deep Learning**\n",
    "\n",
    "# **HW1a The Perceptron** (20 pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vGVmKzgG2Ium",
    "outputId": "4cc2ca21-861a-4fba-a38c-83e3ec04bec8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-17 17:01:59--  http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
      "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
      "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2844 (2.8K)\n",
      "Saving to: ‘test.dat’\n",
      "\n",
      "test.dat            100%[===================>]   2.78K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-02-17 17:02:00 (273 MB/s) - ‘test.dat’ saved [2844/2844]\n",
      "\n",
      "--2024-02-17 17:02:00--  http://huang.eng.unt.edu/CSCE-5218/train.dat\n",
      "Resolving huang.eng.unt.edu (huang.eng.unt.edu)... 129.120.123.155\n",
      "Connecting to huang.eng.unt.edu (huang.eng.unt.edu)|129.120.123.155|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11244 (11K)\n",
      "Saving to: ‘train.dat’\n",
      "\n",
      "train.dat           100%[===================>]  10.98K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-02-17 17:02:00 (131 MB/s) - ‘train.dat’ saved [11244/11244]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the datasets\n",
    "!wget http://huang.eng.unt.edu/CSCE-5218/test.dat\n",
    "!wget http://huang.eng.unt.edu/CSCE-5218/train.dat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A69DxPSc8vNs",
    "outputId": "5440e602-8ecd-44cf-d48d-2e8b00cdcc52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\t\n",
      "1\t1\t0\t0\t0\t0\t0\t0\t1\t1\t0\t0\t1\t0\n",
      "0\t0\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t1\t0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t1\n",
      "0\t0\t1\t0\t0\t1\t0\t1\t0\t1\t1\t1\t1\t0\n",
      "0\t1\t0\t0\t0\t0\t0\t1\t1\t1\t1\t1\t1\t0\n",
      "0\t1\t1\t1\t0\t0\t0\t1\t0\t1\t1\t0\t1\t1\n",
      "0\t1\t1\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t1\t1\t0\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "0\t0\t0\t0\t0\t0\t1\t0\t1\t0\t1\t0\t1\t0\n",
      "A1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\n",
      "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "0\t0\t0\t1\t0\t0\t1\t1\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t1\t1\t0\t1\t1\t1\t1\t0\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t1\t0\t1\t1\t1\t0\t1\t0\t1\t0\n",
      "0\t1\t0\t0\t0\t1\t0\t1\t0\t1\t0\t0\t1\t0\n",
      "0\t1\t1\t0\t0\t1\t1\t1\t1\t1\t1\t0\t1\t0\n",
      "0\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t1\t1\t0\n",
      "0\t1\t0\t0\t1\t0\t0\t1\t1\t0\t1\t1\t1\t0\n",
      "1\t1\t1\t1\t0\t0\t1\t1\t0\t0\t0\t0\t1\t0\n"
     ]
    }
   ],
   "source": [
    "# Take a peek at the datasets\n",
    "!head train.dat\n",
    "!head test.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFXHLhnhwiBR"
   },
   "source": [
    "### Build the Perceptron Model\n",
    "\n",
    "You will need to complete some of the function definitions below.  DO NOT import any other libraries to complete this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cXAsP_lw3QwJ"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import itertools\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Corpus reader, all columns but the last one are coordinates;\n",
    "#   the last column is the label\n",
    "def read_data(file_name):\n",
    "    f = open(file_name, 'r')\n",
    "\n",
    "    data = []\n",
    "    # Discard header line\n",
    "    f.readline()\n",
    "    for instance in f.readlines():\n",
    "        if not re.search('\\t', instance): continue\n",
    "        instance = list(map(int, instance.strip().split('\\t')))\n",
    "        # Add a dummy input so that w0 becomes the bias\n",
    "        instance = [-1] + instance\n",
    "        data += [instance]\n",
    "    return data\n",
    "\n",
    "\n",
    "def dot_product(array1, array2):\n",
    "    #TODO: Return dot product of array 1 and array 2\n",
    "    dot = 0\n",
    "    for i in range(len(array1)):\n",
    "        dot += array1[i] * array2[i]\n",
    "    return dot \n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    #TODO: Return outpout of sigmoid function on x\n",
    "    sigmoid = 1 / (1+math.e** (-x))\n",
    "    return sigmoid\n",
    "\n",
    "# The output of the model, which for the perceptron is \n",
    "# the sigmoid function applied to the dot product of \n",
    "# the instance and the weights\n",
    "def output(weight, instance):\n",
    "    #TODO: return the output of the model\n",
    "    in_values = dot_product(weights, instance)\n",
    "    output = sigmoid(in_values)\n",
    "    return output\n",
    "\n",
    "# Predict the label of an instance; this is the definition of the perceptron\n",
    "# you should output 1 if the output is >= 0.5 else output 0\n",
    "def predict(weights, instance):\n",
    "    #TODO: return the prediction of the model\n",
    "    in_values = dot_product(weights, instance)\n",
    "    output = sigmoid(in_values)\n",
    "    return 1 if output >= 0.5 else 0\n",
    "\n",
    "\n",
    "# Accuracy = percent of correct predictions\n",
    "def get_accuracy(weights, instances):\n",
    "    # You do not to write code like this, but get used to it\n",
    "    correct = sum([1 if predict(weights, instance) == instance[-1] else 0\n",
    "                   for instance in instances])\n",
    "    return correct * 100 / len(instances)\n",
    "\n",
    "\n",
    "# Train a perceptron with instances and hyperparameters:\n",
    "#       lr (learning rate) \n",
    "#       epochs\n",
    "# The implementation comes from the definition of the perceptron\n",
    "#\n",
    "# Training consists on fitting the parameters which are the weights\n",
    "# that's the only thing training is responsible to fit\n",
    "# (recall that w0 is the bias, and w1..wn are the weights for each coordinate)\n",
    "#\n",
    "# Hyperparameters (lr and epochs) are given to the training algorithm\n",
    "# We are updating weights in the opposite direction of the gradient of the error,\n",
    "# so with a \"decent\" lr we are guaranteed to reduce the error after each iteration.\n",
    "def train_perceptron(instances, lr, epochs):\n",
    "\n",
    "    #Initializing weights\n",
    "    weights = [0] * (len(instances[0]))\n",
    "    # print(len(weights))\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for instance in instances:\n",
    "            # print(len(instance))\n",
    "            #Forward Propagation\n",
    "            in_value = dot_product(weights, instance)\n",
    "            output = sigmoid(in_value)\n",
    "            error = instance[-1] - output\n",
    "            #Implementing Backpropagation using loss function and chain rule\n",
    "            for i in range(0, len(weights)):\n",
    "                weights[i] += lr * error * output * (1-output) * instance[i]\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adBZuMlAwiBT"
   },
   "source": [
    "## Run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "50YvUza-BYQF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "lr = 0.005\n",
    "epochs = 5\n",
    "weights = train_perceptron(instances_tr, lr, epochs)\n",
    "accuracy = get_accuracy(weights, instances_te)\n",
    "print(f\"#tr: {len(instances_tr):3}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "      f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBXkvaiQMohX"
   },
   "source": [
    "## Questions\n",
    "\n",
    "Answer the following questions. Include your implementation and the output for each question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCQ6BEk1CBlr"
   },
   "source": [
    "\n",
    "\n",
    "### Question 1\n",
    "\n",
    "In `train_perceptron(instances, lr, epochs)`, we have the follosing code:\n",
    "```\n",
    "in_value = dot_product(weights, instance)\n",
    "output = sigmoid(in_value)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "Why don't we have the following code snippet instead?\n",
    "```\n",
    "output = predict(weights, instance)\n",
    "error = instance[-1] - output\n",
    "```\n",
    "\n",
    "#### Answer\n",
    "To answer this question, we need to understand the output and predict functions clearly. Output function produces the activations of the dot product of weights and instances on the other hand predict function does the same but it does an extra step, that is it marks the instance as 0 (if sigmoid op < 0.5) or 1 (if sigmoid op >= 0.5) based on the sigmoid activation values. But we need the raw activation values to calculate the error of the model which can be used for backpropagation that helps the model to learn its mistakes and adjust the weights accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JU3c3m6YL2rK"
   },
   "source": [
    "### Question 2\n",
    "Train the perceptron with the following hyperparameters and calculate the accuracy with the test dataset.\n",
    "\n",
    "```\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]              # number of epochs\n",
    "lr = [0.005, 0.01, 0.05]              # learning rate\n",
    "```\n",
    "\n",
    "TODO: Write your code below and include the output at the end of each training loop (NOT AFTER EACH EPOCH)\n",
    "of your code.The output should look like the following:\n",
    "```\n",
    "# tr:  20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "# tr:  20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "[and so on for all the combinations]\n",
    "```\n",
    "You will get different results with different hyperparameters.\n",
    "\n",
    "#### TODO Add your answer here (code and output in the format above) \n",
    "\n",
    "#### Code\n",
    "```\n",
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
    "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
    "\n",
    "for lr in lr_array:\n",
    "  for tr_size in tr_percent:\n",
    "    for epochs in num_epochs:\n",
    "      size =  round(len(instances_tr)*tr_size/100)\n",
    "      pre_instances = instances_tr[0:size]\n",
    "      weights = train_perceptron(pre_instances, lr, epochs)\n",
    "      accuracy = get_accuracy(weights, instances_te)\n",
    "      print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "            f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")\n",
    "```\n",
    "#### OUTPUT\n",
    "```\n",
    "#tr: 20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 20, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
    "#tr: 40, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 100, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 100, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 100, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 100, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 77.0\n",
    "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 92.0\n",
    "#tr: 200, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 200, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 80.0\n",
    "#tr: 200, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 97.0\n",
    "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 300, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 300, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 71.0\n",
    "#tr: 300, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 92.0\n",
    "#tr: 300, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 99.0\n",
    "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 400, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 80.0\n",
    "#tr: 400, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 96.0\n",
    "#tr: 400, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 20, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 20, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 20, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 20, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 74.0\n",
    "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 85.0\n",
    "#tr: 40, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
    "#tr: 100, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 100, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 100, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
    "#tr: 100, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 92.0\n",
    "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 98.0\n",
    "#tr: 200, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 200, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 83.0\n",
    "#tr: 200, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 96.0\n",
    "#tr: 200, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 300, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 71.0\n",
    "#tr: 300, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 92.0\n",
    "#tr: 300, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 98.0\n",
    "#tr: 300, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 400, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
    "#tr: 400, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 96.0\n",
    "#tr: 400, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 400, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 20, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 20, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
    "#tr: 20, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 85.0\n",
    "#tr: 20, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 91.0\n",
    "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 91.0\n",
    "#tr: 40, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
    "#tr: 40, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
    "#tr: 40, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 90.0\n",
    "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 97.0\n",
    "#tr: 100, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
    "#tr: 100, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 94.0\n",
    "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 98.0\n",
    "#tr: 100, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 200, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 98.0\n",
    "#tr: 200, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 200, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 200, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 300, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 99.0\n",
    "#tr: 300, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 300, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 300, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 400, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 400, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 400, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 400, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
    "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "G-VKJOUu2BTp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#tr: 20, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 40, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 77.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 92.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 97.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 71.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 92.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 99.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.005; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 96.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.005; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.005; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 20, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 20, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 85.0\n",
      "#tr: 40, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 70.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 92.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 98.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 83.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 96.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 71.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 92.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 98.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.010; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.010; Accuracy (test, 100 instances): 96.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.010; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.010; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.010; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 20, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 20, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 74.0\n",
      "#tr: 20, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 85.0\n",
      "#tr: 20, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 91.0\n",
      "#tr: 20, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 91.0\n",
      "#tr: 40, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 68.0\n",
      "#tr: 40, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 78.0\n",
      "#tr: 40, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 90.0\n",
      "#tr: 40, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 97.0\n",
      "#tr: 100, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 80.0\n",
      "#tr: 100, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 94.0\n",
      "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 98.0\n",
      "#tr: 100, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 100, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 200, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 98.0\n",
      "#tr: 200, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 200, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 200, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 200, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 300, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 99.0\n",
      "#tr: 300, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 300, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 300, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 300, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 400, epochs:   5, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 400, epochs:  10, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 400, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 400, epochs:  50, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n",
      "#tr: 400, epochs: 100, learning rate: 0.050; Accuracy (test, 100 instances): 100.0\n"
     ]
    }
   ],
   "source": [
    "instances_tr = read_data(\"train.dat\")\n",
    "instances_te = read_data(\"test.dat\")\n",
    "tr_percent = [5, 10, 25, 50, 75, 100] # percent of the training dataset to train with\n",
    "num_epochs = [5, 10, 20, 50, 100]     # number of epochs\n",
    "lr_array = [0.005, 0.01, 0.05]        # learning rate\n",
    "\n",
    "for lr in lr_array:\n",
    "  for tr_size in tr_percent:\n",
    "    for epochs in num_epochs:\n",
    "      size =  round(len(instances_tr)*tr_size/100)\n",
    "      pre_instances = instances_tr[0:size]\n",
    "      weights = train_perceptron(pre_instances, lr, epochs)\n",
    "      accuracy = get_accuracy(weights, instances_te)\n",
    "      print(f\"#tr: {len(pre_instances):0}, epochs: {epochs:3}, learning rate: {lr:.3f}; \"\n",
    "            f\"Accuracy (test, {len(instances_te)} instances): {accuracy:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFB9MtwML24O"
   },
   "source": [
    "### Question 3\n",
    "Write a couple paragraphs interpreting the results with all the combinations of hyperparameters. Drawing a plot will probably help you make a point. In particular, answer the following:\n",
    "- A. Do you need to train with all the training dataset to get the highest accuracy with the test dataset?\n",
    "- B. How do you justify that training the second run obtains worse accuracy than the first one (despite the second one uses more training data)?\n",
    "```\n",
    "#tr: 100, epochs:  20, learning rate: 0.050; Accuracy (test, 100 instances): 71.0\n",
    "#tr: 200, epochs:  20, learning rate: 0.005; Accuracy (test, 100 instances): 68.0\n",
    "```\n",
    "- C. Can you get higher accuracy with additional hyperparameters (higher than `80.0`)?\n",
    "- D. Is it always worth training for more epochs (while keeping all other hyperparameters fixed)?\n",
    "\n",
    "#### TODO: Add your answer here (code and text)\n",
    "\n",
    "#### Answer A\n",
    "It is not always necessary to train the model with all the training dataset, it depends on the complexity of the model and the amount of data we have in the dataset. This has clearly been proven from above code block. But we may need to train the model for longer with less data to achieve better results. We were able to achive 100% accuracy with only 200 data points by adjusting learning rate. So, we not always need to train the model with the entire dataset \n",
    "\n",
    "#### Answer B\n",
    "If we use the same set of parameters we will always get the same results unless we use randon intializations (which can be solved using a common seed value). But this is only true if the size of the dataset is same, but if the size of the dataset changes this may affect the hyperparameters and we may see some different results compared to the initial run. If the data is good then we may see better results or bad results if the second part of the dataset is not good. Also if the hyperparameters are not chosen correctly for the second run can result in bad results. It is always a good idea to play with different values to find the best hyperparameters.\n",
    "\n",
    "#### Answer C\n",
    "In the model that we used in this task it is not necessary to introduce new parameters, since we were able to achieve 100% accuracy just by tuning the size of dataset, learning rate and epochs. But it may be tru in some cases where we deal with complex problems or reasearch activities. \n",
    "\n",
    "#### Answer D\n",
    "It totally depends on the observations made while testing. If the hyperparameters are selected correctly and the model is doing well and reaches 100% accuracy at 100 epochs then it is not necessary to train the model for 500 epoochs, since it has already reached the saturation point and training any further wont do any good. So, to find this saturation point we should experiment with different epoch values. In reality, instead of depending on number of epochs it is good to stop the training based on a particular metric, since we dont know how long we need to train the model to reach that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW2_The_Perceptron.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
